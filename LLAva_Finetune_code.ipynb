{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsurikaDilrukshi/Research_LLM_Finetune/blob/main/LLAva_Finetune_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RnUAI5YsPOv",
        "outputId": "e488476a-65f5-4699-f653-8dd2b83ecf23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrO-ctI5sSUl"
      },
      "outputs": [],
      "source": [
        "dataset_dir = \"/content/drive/MyDrive/Research/Solar image data/images\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oDHBAzEsbnU"
      },
      "source": [
        " Step 3: Define Fault Descriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EtRLbtvazv5L",
        "outputId": "895efdfd-7e4a-430e-acfd-9cf8c5e3563e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Collecting transformers>=4.39.0\n",
            "  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.39.0) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.39.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.39.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.39.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.39.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.39.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.39.0) (2025.7.9)\n",
            "Downloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.53.1\n",
            "    Uninstalling transformers-4.53.1:\n",
            "      Successfully uninstalled transformers-4.53.1\n",
            "Successfully installed transformers-4.53.2\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.53.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.8.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.33.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.7.9)\n",
            "Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.46.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting trl>=0.8.3\n",
            "  Downloading trl-0.19.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from trl>=0.8.3) (1.8.1)\n",
            "Collecting datasets>=3.0.0 (from trl>=0.8.3)\n",
            "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: transformers>=4.51.0 in /usr/local/lib/python3.11/dist-packages (from trl>=0.8.3) (4.53.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl>=0.8.3) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl>=0.8.3) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl>=0.8.3) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl>=0.8.3) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl>=0.8.3) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl>=0.8.3) (0.33.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl>=0.8.3) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl>=0.8.3) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl>=0.8.3) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl>=0.8.3) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl>=0.8.3) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl>=0.8.3) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl>=0.8.3) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl>=0.8.3) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl>=0.8.3) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.8.3)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.0->trl>=0.8.3) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.0->trl>=0.8.3) (0.21.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.8.3) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl>=0.8.3) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl>=0.8.3) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl>=0.8.3) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl>=0.8.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl>=0.8.3) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl>=0.8.3) (2025.7.9)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl>=0.8.3) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl>=0.8.3) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl>=0.8.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl>=0.8.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl>=0.8.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl>=0.8.3) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl>=0.8.3) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl>=0.8.3) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl>=0.8.3) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl>=0.8.3) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl>=0.8.3) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl>=0.8.3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl>=0.8.3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl>=0.8.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl>=0.8.3) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl>=0.8.3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl>=0.8.3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=1.4.0->trl>=0.8.3) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl>=0.8.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl>=0.8.3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl>=0.8.3) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.8.3) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.8.3) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.8.3) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.8.3) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.8.3) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.8.3) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.8.3) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl>=0.8.3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl>=0.8.3) (3.0.2)\n",
            "Downloading trl-0.19.1-py3-none-any.whl (376 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets, trl\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-4.0.0 fsspec-2025.3.0 trl-0.19.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U \"transformers>=4.39.0\"\n",
        "!pip install peft bitsandbytes\n",
        "!pip install -U \"trl>=0.8.3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmugusCosVx2"
      },
      "outputs": [],
      "source": [
        "FAULT_DESCRIPTIONS = {\n",
        "    \"clean\": \"This is a clean solar panel in good condition. There are no visible signs of dirt, damage, or obstruction. It should function at optimal efficiency.\",\n",
        "    \"dusty\": \"This solar panel is covered in dust. Dust accumulation blocks sunlight from reaching the cells, which significantly reduces energy output. Regular cleaning is required to maintain performance.\",\n",
        "    \"physical damage\": \"The solar panel shows physical damage, such as cracks or broken glass. This can lead to decreased performance and may even pose safety risks due to short-circuiting or exposure to weather.\",\n",
        "    \"bird-drops\": \"Bird droppings are visible on this panel. These block sunlight and can cause hotspots that damage the cells over time. The panel should be cleaned as soon as possible.\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXRbjJvEseIu"
      },
      "source": [
        "Step 4: Create Prompt-Answer Pairs for All Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jiC2v4NsgK1",
        "outputId": "49acad61-b310-4219-ecf3-9f575a52643c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Prepared 218 examples for fine-tuning\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def prepare_llava_dataset(dataset_dir):\n",
        "    data = []\n",
        "\n",
        "    for category in os.listdir(dataset_dir):\n",
        "        category_dir = os.path.join(dataset_dir, category)\n",
        "\n",
        "        if not os.path.isdir(category_dir):\n",
        "            continue  # skip if it's not a folder\n",
        "\n",
        "        # Get the fault description from the dictionary\n",
        "        if category.lower() not in FAULT_DESCRIPTIONS:\n",
        "             print(f\"⚠️ Skipping unknown category: {category}\")\n",
        "             continue\n",
        "\n",
        "        description = FAULT_DESCRIPTIONS[category.lower()]\n",
        "\n",
        "\n",
        "        for img_name in os.listdir(category_dir):\n",
        "            if not img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                continue  # skip non-image files\n",
        "\n",
        "            img_path = os.path.join(category_dir, img_name)\n",
        "\n",
        "            try:\n",
        "                image = Image.open(img_path).convert(\"RGB\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Skipping {img_path} due to error: {e}\")\n",
        "                continue\n",
        "\n",
        "            # Construct the LLaVA-style prompt-answer format\n",
        "            example = {\n",
        "                \"messages\": [\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": [\n",
        "                            {\"type\": \"image\"},\n",
        "                            {\"type\": \"text\", \"text\": \"Describe the fault in this solar panel image in detail. Explain what caused it and how it affects performance\"}\n",
        "                        ]\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": [\n",
        "                            {\"type\": \"text\", \"text\": description}\n",
        "                        ]\n",
        "                    }\n",
        "                ],\n",
        "                \"images\": [image]  # the loaded PIL image\n",
        "            }\n",
        "\n",
        "            data.append(example)\n",
        "\n",
        "    print(f\" Prepared {len(data)} examples for fine-tuning\")\n",
        "    return data\n",
        "\n",
        "# Generate dataset\n",
        "llava_training_data = prepare_llava_dataset(dataset_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im4B9E1ZsoIU"
      },
      "source": [
        "You can preview the prompt-answer structure for the first 2-3 samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naVvt0qaslu0",
        "outputId": "5284a4d7-53e7-4d32-c0cc-35e326acc26c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 1\n",
            "User Prompt: Describe the fault in this solar panel image in detail. Explain what caused it and how it affects performance\n",
            "Assistant Answer: Bird droppings are visible on this panel. These block sunlight and can cause hotspots that damage the cells over time. The panel should be cleaned as soon as possible.\n",
            "\n",
            "Sample 2\n",
            "User Prompt: Describe the fault in this solar panel image in detail. Explain what caused it and how it affects performance\n",
            "Assistant Answer: Bird droppings are visible on this panel. These block sunlight and can cause hotspots that damage the cells over time. The panel should be cleaned as soon as possible.\n",
            "\n",
            "Sample 3\n",
            "User Prompt: Describe the fault in this solar panel image in detail. Explain what caused it and how it affects performance\n",
            "Assistant Answer: Bird droppings are visible on this panel. These block sunlight and can cause hotspots that damage the cells over time. The panel should be cleaned as soon as possible.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, sample in enumerate(llava_training_data[:3]):\n",
        "    print(f\"Sample {i+1}\")\n",
        "    print(\"User Prompt:\", sample[\"messages\"][0][\"content\"][1][\"text\"])\n",
        "    print(\"Assistant Answer:\", sample[\"messages\"][1][\"content\"][0][\"text\"])\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaZ7fRWryEeK",
        "outputId": "673b42e2-0277-4a29-a70d-d9e8f2172652"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Train samples: 152\n",
            " Eval samples: 32\n",
            " Test samples: 34\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "random.shuffle(llava_training_data)\n",
        "\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "n_total = len(llava_training_data)\n",
        "n_train = int(train_ratio * n_total)\n",
        "n_val = int(val_ratio * n_total)\n",
        "\n",
        "train_data = llava_training_data[:n_train]\n",
        "val_data = llava_training_data[n_train:n_train + n_val]\n",
        "test_data = llava_training_data[n_train + n_val:]\n",
        "\n",
        "\n",
        "print(f\" Train samples: {len(train_data)}\")\n",
        "print(f\" Eval samples: {len(val_data)}\")\n",
        "print(f\" Test samples: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a9VzQWKyUWl"
      },
      "source": [
        "Define the PyTorch Dataset class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iws6Z2C0yhJH"
      },
      "source": [
        "Wrap your split lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTnQpDSVyjz2"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from datasets import Dataset as hfDataset # Import the Hugging Face Dataset\n",
        "\n",
        "class LlavaSolarDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data  # a list of dicts with \"messages\" and \"images\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "train_dataset = LlavaSolarDataset(train_data)\n",
        "val_dataset = LlavaSolarDataset(val_data)\n",
        "test_dataset = LlavaSolarDataset(test_data)\n",
        "\n",
        "#Convert to Hugging Face Dataset format\n",
        "train_dataset_hf = hfDataset.from_list(train_data)\n",
        "val_dataset_hf = hfDataset.from_list(val_data)\n",
        "test_dataset_hf = hfDataset.from_list(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2UT6N2QynRr"
      },
      "source": [
        "Sanity Check a Few Samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHXDzt4xztzt"
      },
      "source": [
        "3.1 Install Required Packages\n",
        "\n",
        "These libraries enable 4-bit quantization, LoRA, and LLaVA support."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmxjP_Z7zzwG"
      },
      "source": [
        " 3.2 Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kWOU1JMz3R3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "AutoTokenizer,\n",
        "AutoProcessor,\n",
        "LlavaForConditionalGeneration,\n",
        "BitsAndBytesConfig\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLyRQf1iz7ve"
      },
      "source": [
        "3.3 Set Model ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PASrdFyY0DNT"
      },
      "outputs": [],
      "source": [
        "model_id = \"llava-hf/llava-1.5-7b-hf\" # Or: \"llava-hf/llava-v1.6-mistral-7b-hf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRrVxqny0Gwg"
      },
      "source": [
        " 3.4 Define BitsAndBytesConfig for 4-bit Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xPkh48X0Isp"
      },
      "outputs": [],
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "load_in_4bit=True,\n",
        "bnb_4bit_compute_dtype=torch.float16,\n",
        "bnb_4bit_use_double_quant=True,\n",
        "bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8571hvL80LPg"
      },
      "source": [
        " 3.5 Load the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402,
          "referenced_widgets": [
            "92488e40951f417e8587ddb8223175eb",
            "d4e55a94dbf04e539369401f1fda4215",
            "9d1d0664979147b3aae0b6b96c76a881",
            "bb2f2d83a1b8431cb01ccd2c355da754",
            "5db030fd07314eef9fc07d85c9ed63b1",
            "848e13a35cde47d28b1ae37fd5b047a7",
            "51644407fa2646478c0e55219b1d49e8",
            "41223808ccc746e98908838230e7a845",
            "76eaf6468f4f469dbf2888588c0fa888",
            "b1f46bfdba26401f99f2107b54c5fcd1",
            "092bdd0cf32c4cda9dacd71fc84cd93b",
            "4bb6b865b66b48e29280b804fe0ac2a7",
            "3ddcf53548824c419d63cd6b8679f0d8",
            "b786d0d4036a41319f86928ef4719169",
            "3dac4ff8b4cf41f0ae1fb52a7cacb203",
            "e323257a8d0f495585a21618a3b6e1d3",
            "22ffb32245824818bbefa3e877d6d6f7",
            "0fd83e8e1b484488bb27dc9e02a04a93",
            "6ec1ca808c3c41958449de6c10d548eb",
            "64c57a043d7d46f4ab7698470a2d92f2",
            "2b54db1ff96343d9a8c75b57003e9b5d",
            "461387cb45124669b6c7987e1c843fbe",
            "b1ad5372284d43e6ba544177c6c63fce",
            "dc9c2de2f402406ea14850a0b61d809b",
            "ad6c25a45f4243e59747971110d81b45",
            "b99bbf642e2e4bc283bdd441055f5509",
            "da5342958d70460e87b27400d55acf76",
            "e40c37fd0da141088ac0515a71629ea2",
            "5661d18e755143498b4a7e10b7646cc8",
            "dc308600aa3e4a2b9e13d0dfa52a744e",
            "46b4b8c9613849bcbfe3fde48083ef56",
            "e4c7d3f8864c4cfb837881d3c90ca30a",
            "f6e3d2e85abe4347ac396077d91893d4",
            "7d28c9ab18e14053bbea83eef967b29a",
            "97be358cbc2f405993dcdf33c1af683a",
            "f09ab6bac0c44d8abdf1c95624c19a8c",
            "f41bf52b04534bf2ae2a6906cf0af46b",
            "42b340c68053483fa5e1c38b4bd077ef",
            "7cbe8eb96b844904a6ab7eab7d1e1a43",
            "8901422daf094db390983ab6c06f1940",
            "c3bfd6f5e4e94a5f91a0ded4934a0773",
            "76412a7a0d1b43c791e7c4c7286a632d",
            "a7b3ff7c105640fcb4e602742e6b838f",
            "8e8eba44bee64519a99fae411500aae9",
            "835dc3ef1a6644f793a5869ab61cd95a",
            "fd151e9de95b40c7aadd6fbe789567e7",
            "fc6e92059f99490581f6d3001455f604",
            "d291cadd931141b989ac4afb1545e7bf",
            "58257a9943e74aefb7753030f37eb1fa",
            "26b9980cdb5f4778bab73c952dc146aa",
            "08b4c5625d3c4040993289b59213450a",
            "d3579cb6dac64fe3a10605b91b00d6d9",
            "44a1973eb5424a3b9dd756a3ff40ef12",
            "0860666bb1ef49c78db69d101d3c62a0",
            "5900e51c62a4409393ee778f205f7b49",
            "df7a72e886c2404d88e5226a608b4417",
            "6b29ff4e16234a1b9a843dc073fb9711",
            "123689a9670145cbbf167614dc9b621d",
            "dcb36a003dfb46a581f01c388fc80c4e",
            "5ccfb2e5ea8043909501183b4dbe3201",
            "e37535a058f748ea8e68c8975e269807",
            "f66cb179a4e2469194bb222e0087fbb0",
            "8e1daedd6bc540b68ccd51a38a8b90b0",
            "a60bfef89e094a6ea566cc164d95c4e1",
            "7fa04477fa494ea9928427bf3ba965c8",
            "3c9f1a71f5fe4601a4db76722312b1e0",
            "c93e1738f92a4a838312e373bf078fac",
            "2e45adb3d45a47a893608b7c45c11d3b",
            "09e682d1a2244b32b47eaff7d3f25bfd",
            "347ac04a73c04220af543c96b6305bb1",
            "0a7a1f19565a4a938c0bf2bec10b8c50",
            "d0d9f10b04c84fef82c281bda9e12490",
            "630be58b55c0437693a25a9342c43e46",
            "cbe33b6ad54e446894cdc07c4834ad3e",
            "b207b528b48f4c488cce1b82b98a1030",
            "4ab578f9be6341b8801595a859fa675b",
            "953b30241eb2469b8ac5f840b979f34f",
            "6ad2fb859f33426d99fbaddcdaec6d2d",
            "125cd4c697004009982d52f31c549d9b",
            "ba45c7f2af254e31980e1ca54a264179",
            "ab7e9fff5dc7493180da02cae61ea32b",
            "489f1696b04148448c213b6c1145c397",
            "17cfd6a642ab4f3f9ef4f408f8d4235f",
            "34c09887614b403993186d8b6f634908",
            "fd205d7edc2940c4905dd26c9d333c2f",
            "f50a41d5a3e043cba7153b162ed56008",
            "0287c60bd16047c5a312ffe98b75819a",
            "ac5cd6345b3f4be9bd2e4427ba463cd0"
          ]
        },
        "id": "6ORVTFFV0Nd_",
        "outputId": "f9b2dd8f-d9ce-4737-ab13-e10e8ed67277"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92488e40951f417e8587ddb8223175eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/950 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4bb6b865b66b48e29280b804fe0ac2a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1ad5372284d43e6ba544177c6c63fce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d28c9ab18e14053bbea83eef967b29a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "835dc3ef1a6644f793a5869ab61cd95a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df7a72e886c2404d88e5226a608b4417",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/4.18G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c93e1738f92a4a838312e373bf078fac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ad2fb859f33426d99fbaddcdaec6d2d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = LlavaForConditionalGeneration.from_pretrained(\n",
        "model_id,\n",
        "torch_dtype=torch.float16,\n",
        "quantization_config=quant_config,\n",
        "device_map=\"auto\" # Automatically assigns layers to available GPUs/CPUs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldICgwVY0Ppm"
      },
      "source": [
        "3.6 Load Tokenizer and Processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343,
          "referenced_widgets": [
            "85a85bea98184d159051e394ea304ec7",
            "da97b57816394eaabbd1c3e495723cfe",
            "d4e6977ad998413688aa67b584ce619e",
            "12e5146bc4d845c9ae694eeead7d8765",
            "65ac3323694b4088bfbca14ea057217e",
            "915f133841d5459584a0c87abe0404ef",
            "23f33d7424534a429485a41ba7eb0800",
            "b09e9050058c4c8b8029f9a10f57947a",
            "a1c84d3caf70495bbe08422b495384ac",
            "0f4a81f8a36140e2ad8bddcbe6eddc7b",
            "40b65ecf53244d0f842933771a499bc7",
            "f285a24c921a46ee8acbccc48122f5c9",
            "d65df7cd867d46baa07e9ec08e611852",
            "339c4d20c5da41368eb9dc09ec44fc79",
            "8683e28d03234cb8b032e5d5dd19753a",
            "ec518d1fab5c452989c96d7a9446fb6e",
            "466d3ac577914e1f80ae0cc4e11626c1",
            "9dc6575e4d6b4eef9dcee99af8736efa",
            "61fcde2501844cd39c275877677984cf",
            "ca84a38e1b6346fa8a877f5ee6349752",
            "e23bfea5b6774b8a9e88f81233f3afa3",
            "481d89737bc845008ad06804bfd59581",
            "8f9fd7c62e5d48e1967d26a98ceffc04",
            "0a74363df3fe4411a13e6445cce50160",
            "8fb38168d6e34c10baeb6246061ad7b0",
            "4e8d55ea1cf34a578ccecd85db626a11",
            "184f93af9ea1460d81ca1c3bd0ba0ad4",
            "41b9ba3ebf5f4bff84306b51771950e3",
            "ca9cdfcb12084993b70fa39eb2eb5ad5",
            "d098a49876494d6386243f25b5811e33",
            "859720ce8c9248d3af6ad712289280c5",
            "8f05d6e908354a75b8405beb1bb4e73a",
            "a8ec8b19f44644f1bc224d0710648632",
            "8334b9dbfede4436bfdcf273448cfcef",
            "82610efd49d349f08971d78838abe018",
            "86ce4eb1ef864cd390f14147fda553c2",
            "038be5952b294e08a0c29250e44af8c3",
            "fb219dd1d56c4cccb4ee87ecfdff477f",
            "d23a6ea41c464ac88bb552ad1b28c52a",
            "8364749a561f49f6aee347888e8f41d2",
            "bee21b6a5db74aca910d4c0add10c311",
            "1364d147e5de4a84a5bfb8540456c65d",
            "c3e416b482654443bf7c0d4ff1a59570",
            "8fe34e70b0d34e358376d58dda3a2e11",
            "98b8075ab65349e9960c7834fb95f85c",
            "0466f2b1cd2b4726a71cd81c197642e5",
            "4895aee4723e41be9281471b1d1b7382",
            "cf10a425aa7a4c56b39fbc2adf4bb943",
            "3a8b28f3e9e74bcca7c11daa1f98f6ee",
            "80759ee4323a4f84868bc232802da407",
            "4af41f8190944b81b5ab7cf58cdb2f80",
            "60993a9a65fa4239bba236d87eac2172",
            "784fa480f12848b490acefcc27f4b71b",
            "1901497ccb0149dba7ed06acd959d0d9",
            "88817b10dab64944b060489c7cfa557d",
            "1f211cd9558e47d69a992d5a6a438a53",
            "fc82bdee43174e16861df3a090aef5f9",
            "f9175443c44048a39980d44a49efee71",
            "a785d6511f3f4a6fbab2125cf77018b9",
            "5627c5805a164fa29f87a5f9ffeb5ef8",
            "250b63244b804c0fa596f83eb6feea18",
            "6656af8965a54ecd8fd968543a1d7f35",
            "edc7faa553ea451ea1cc573b67f10d3d",
            "5d157a653fbb4074875cc9a96ed4db40",
            "60bbc864f4484e64921ef1ab9f1b9282",
            "657834d1d70f43d6987666d99c71147c",
            "3c12eef80cf443268cf36548e49e5ce3",
            "8e39f5bf19b24e728d15986c1f05a2c3",
            "7d84c16216dc42cfbfb8a9fff528e526",
            "8087cb9a8bd1491a9328b92029ec8d53",
            "f3ec6005cb8546dba8c95a800b31463b",
            "72b569c56b3e4d22a227e218de471fd5",
            "ff7819a5ccd04b8ba38e4d3a882656a6",
            "b5b046aa5afe4a4a87b1c064f53010a5",
            "9b8e9eb6c92a48d38c0e00c49cae7672",
            "c936362f29e042499030840326279309",
            "f484ad5366eb42e8bedc47daac719e32",
            "392f98df8776490eb9af5fcc66f7baa7",
            "98dfdbc3610a481fb44e736825686e37",
            "1437ba749921448cab84c9cd4346a766",
            "ba01502b7ccc43e1bd151a1e5888983b",
            "1b8094e8589b474fa1f4b891b2aee9db",
            "228f8838fbf645f9a49d3cce71fe4542",
            "922cd1b463814df2b475dd0a549b1f85",
            "e93b81d7f00243f79e7fe754f9794bb5",
            "8e81f64655004003890eb5d5752caabb",
            "2e6663b001bb4e22911499254755d6a7",
            "9ce387bfafbe4ceda8b85d33050119fe",
            "84ba9a89ada9486eadac1fab1f4ada35",
            "826c432c2bb04247b13e70fe1131db65",
            "4bdc6c37edfd42da954e5a42e5fbdd12",
            "7394fa5e46214b1888d4c4becbb5fb25",
            "5374fe2c48e44a6aa0d39c7a8a9cac79",
            "8426ab9ee1ef4d15b955a8d2de1a16c0",
            "2b6894ba76ec45b6900a39c27dd83e38",
            "afb0864442af4634b5ed0a81044ce826",
            "4b0b04c224b5430ca085f8ee7c2fe841",
            "9e0d5c6a61734b79955f73c85591a0b7",
            "1217c4b4135c40acbe1f8a78621db129"
          ]
        },
        "id": "gwTwuvqO0RPY",
        "outputId": "5e69a268-c71c-4711-bfbd-19955f74d8c9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85a85bea98184d159051e394ea304ec7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f285a24c921a46ee8acbccc48122f5c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f9fd7c62e5d48e1967d26a98ceffc04",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8334b9dbfede4436bfdcf273448cfcef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98b8075ab65349e9960c7834fb95f85c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f211cd9558e47d69a992d5a6a438a53",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chat_template.jinja:   0%|          | 0.00/674 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c12eef80cf443268cf36548e49e5ce3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "processor_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "392f98df8776490eb9af5fcc66f7baa7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chat_template.json:   0%|          | 0.00/701 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84ba9a89ada9486eadac1fab1f4ada35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/505 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "processor = AutoProcessor.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K6k2jWh0TFz"
      },
      "source": [
        " 3.7 (Optional but Important) Define Chat Template for LLaVA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71_M2plA0Wjt"
      },
      "outputs": [],
      "source": [
        "LLAVA_CHAT_TEMPLATE = \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. {% for message in messages %}{% if message['role'] == 'user' %}USER: {% else %}ASSISTANT: {% endif %}{% for item in message['content'] %}{% if item['type'] == 'text' %}{{ item['text'] }}{% elif item['type'] == 'image' %}<image>{% endif %}{% endfor %}{% if message['role'] == 'user' %} {% else %}{{eos_token}}{% endif %}{% endfor %}\"\"\"\n",
        "\n",
        "tokenizer.chat_template = LLAVA_CHAT_TEMPLATE\n",
        "processor.tokenizer = tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkHrHKweC5K5"
      },
      "source": [
        "Step 4: Add Trainable LoRA Adapter (Required for 4-bit Finetuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpR83m2iU_8x",
        "outputId": "8f258a78-88d9-43d9-8e3d-6a2dbff2017b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 76,546,048 || all params: 7,139,973,120 || trainable%: 1.0721\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)  # ✅ Important for 4-bit models\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruIvcRgTDM0x"
      },
      "source": [
        " 4.2 Attach LoRA Adapter to the Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRyXvpv6DPjz",
        "outputId": "0a1ae938-5f35-4360-d83d-0a27e0ca9dd5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'llava-hf/llava-1.5-7b-hf' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEADlLwgDU9K",
        "outputId": "9bf6d4ad-c08a-4130-d48e-f2daf14fd983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Trainable parameters: 76546048 / 3740489728 (2.05%)\n"
          ]
        }
      ],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"✅ Trainable parameters: {trainable} / {total} ({100 * trainable / total:.2f}%)\")\n",
        "\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sr2-PI6fJOqa"
      },
      "outputs": [],
      "source": [
        "class LlavaDataCollator:\n",
        "    def __init__(self, processor):\n",
        "        self.processor = processor  # includes tokenizer and image processor\n",
        "\n",
        "    def __call__(self, examples):\n",
        "        texts = []\n",
        "        images = []\n",
        "\n",
        "        for example in examples:\n",
        "            messages = example[\"messages\"]\n",
        "\n",
        "            # Convert messages (chat) to a single text prompt using the chat template\n",
        "            text = self.processor.tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=False  # We're providing ground truth\n",
        "            )\n",
        "\n",
        "            texts.append(text)\n",
        "            images.append(example[\"images\"][0])  # First image per sample\n",
        "\n",
        "        # Tokenize prompts and preprocess images - Corrected order\n",
        "        batch = self.processor(\n",
        "            images,  # Corrected: images first\n",
        "            texts,   # Corrected: texts second\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True\n",
        "        )\n",
        "\n",
        "        # Create labels from input_ids\n",
        "        labels = batch[\"input_ids\"].clone()\n",
        "\n",
        "        # Set pad_token_id as -100 in labels to ignore during loss\n",
        "        if self.processor.tokenizer.pad_token_id is not None:\n",
        "            labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-4ijgnFJbBE"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "data_collator = LlavaDataCollator(processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0BnigGeJrxX"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=2, collate_fn=data_collator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ35KRDwK1h4"
      },
      "source": [
        "Step 5: Set Up Training Configuration (TrainingArguments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsxpYaGoK49q"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "output_dir=\"./llava-solar-ft\", # Folder to save model checkpoints\n",
        "per_device_train_batch_size=1, # Adjust based on your GPU memory\n",
        "per_device_eval_batch_size=1,\n",
        "gradient_accumulation_steps=1, # Effective batch size = 2 × 2 = 4\n",
        "learning_rate=2e-4, # Start with 2e-4 or 1e-4\n",
        "num_train_epochs=10, # You can increase this for better results\n",
        "logging_steps=10, # Print loss every 10 steps\n",
        "save_steps=100, # Save checkpoint every 100 steps (optional)\n",
        "eval_strategy=\"steps\", # Evaluate during training (optional)\n",
        "eval_steps=50, # How often to evaluate (optional)\n",
        "save_total_limit=2, # Keep only last 2 checkpoints (optional)\n",
        "fp16=True, # Enable 16-bit training (saves memory)\n",
        "remove_unused_columns=False, # Needed for custom collator\n",
        "report_to=\"tensorboard\", # Enable TensorBoard logging\n",
        "logging_dir=\"./logs\", # Log dir for TensorBoard\n",
        "gradient_checkpointing=True # Reduce memory usage\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183,
          "referenced_widgets": [
            "0af1fff70e294eb79f037c96ea1ba30e",
            "19160d7f0530490cb094088eba65c346",
            "80d82345599742a7a33854a48d73690a",
            "d921a1ea19e942c4b844b2b40a3e4e21",
            "905da3818d4a40de849f336d5727e563",
            "33485f499e814ab9805f2b8203b6f1ed",
            "80f5982d9e714ef1975855b76ee5b696",
            "802c5eed9f0a4014b8be8b6056115a3b",
            "3172834dcde74718a007d84d47dfdb36",
            "d81e237d64c64589ba833ce94703006e",
            "bb1e765ed30447b4be32de1a27371d9b",
            "bb84bcbe350a49829c14a9d3a2a64095",
            "c402ca2d354c459f812951e77ca77671",
            "789b2fb6e4e045219d13896ae2534326",
            "218e8d0e359b4b6ea52e0ff9810ff931",
            "1c81cec7d64942c2a446979c809f5a07",
            "75d109ed7f214448941b81dbdc2236cc",
            "e7619270521341c1a99d0db35886f883",
            "0dc114a91f30426cb2235a66e2cd7ac8",
            "122e2df8bd534d9bbbeaeea8ee83c973",
            "f04358eceac748e0aadb62f866b3f6b4",
            "d256b27415894942a5f5699ef8ba78e2",
            "43467401d6ae4ba3b47d0e2c10b5758b",
            "0ab382c357034fc7a87a3d2b084f75d0",
            "bc674ff4e69a4dd8b90a2e2d39e61bf6",
            "6ed34736ee28465bb5edf53884c083e9",
            "fcf36f08ca964591ad47eebfe0a2fd8a",
            "ef466d9213ad4e689586ad4d2930be81",
            "316414480bde4ec9a3249ae6514754b1",
            "baaa55b82cfa4979b9f680cc42b1a924",
            "21dd4f330ef44874af9ce23826a10a6b",
            "b17716a83cec40878d30cd926964619b",
            "fa23aeff316840aaaea06c21635780c4",
            "dc39f1d147c24c4aafe5c078bbcae14a",
            "cc974cbec81545a38955bbafbb2c032b",
            "26a015a9a232478682d45cfb719ff3bb",
            "9a871dcacbd4446cacfefb39bec173d9",
            "a6d6005887f74f87b351d247e1830f7b",
            "cd0f15ee0183430988d4ba1ccfe4a7c7",
            "962d0ed66f3e42c0884808924416a92f",
            "481e1b2da7af4304b4977c69eaaf3814",
            "3ba526b9c37f42c19100ecc1d155a5a0",
            "4a5079414e974bc595d22b2de769ddad",
            "f47ad72dddb24f6e8dfd682f6f93ff79"
          ]
        },
        "id": "6nv1B7tMNFOm",
        "outputId": "dc018244-0b2e-4b55-9e67-7f8feb329833"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0af1fff70e294eb79f037c96ea1ba30e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/152 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb84bcbe350a49829c14a9d3a2a64095",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/152 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43467401d6ae4ba3b47d0e2c10b5758b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing eval dataset:   0%|          | 0/32 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc39f1d147c24c4aafe5c078bbcae14a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating eval dataset:   0%|          | 0/32 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from datasets import Dataset # Import Dataset from datasets library\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset_hf, # Use the Hugging Face Dataset\n",
        "    eval_dataset=val_dataset_hf,   # Use the Hugging Face Dataset\n",
        "    data_collator=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QzNH5eTUUkMk",
        "outputId": "aba39ab6-92e0-4495-df94-e3c4b04badfd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): PeftModelForCausalLM(\n",
              "      (base_model): LoraModel(\n",
              "        (model): LlavaForConditionalGeneration(\n",
              "          (model): LlavaModel(\n",
              "            (vision_tower): CLIPVisionModel(\n",
              "              (vision_model): CLIPVisionTransformer(\n",
              "                (embeddings): CLIPVisionEmbeddings(\n",
              "                  (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
              "                  (position_embedding): Embedding(577, 1024)\n",
              "                )\n",
              "                (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "                (encoder): CLIPEncoder(\n",
              "                  (layers): ModuleList(\n",
              "                    (0-23): 24 x CLIPEncoderLayer(\n",
              "                      (self_attn): CLIPAttention(\n",
              "                        (k_proj): lora.Linear4bit(\n",
              "                          (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "                          (lora_dropout): ModuleDict(\n",
              "                            (default): Dropout(p=0.1, inplace=False)\n",
              "                          )\n",
              "                          (lora_A): ModuleDict(\n",
              "                            (default): Linear(in_features=1024, out_features=64, bias=False)\n",
              "                          )\n",
              "                          (lora_B): ModuleDict(\n",
              "                            (default): Linear(in_features=64, out_features=1024, bias=False)\n",
              "                          )\n",
              "                          (lora_embedding_A): ParameterDict()\n",
              "                          (lora_embedding_B): ParameterDict()\n",
              "                          (lora_magnitude_vector): ModuleDict()\n",
              "                        )\n",
              "                        (v_proj): lora.Linear4bit(\n",
              "                          (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "                          (lora_dropout): ModuleDict(\n",
              "                            (default): Dropout(p=0.1, inplace=False)\n",
              "                          )\n",
              "                          (lora_A): ModuleDict(\n",
              "                            (default): Linear(in_features=1024, out_features=64, bias=False)\n",
              "                          )\n",
              "                          (lora_B): ModuleDict(\n",
              "                            (default): Linear(in_features=64, out_features=1024, bias=False)\n",
              "                          )\n",
              "                          (lora_embedding_A): ParameterDict()\n",
              "                          (lora_embedding_B): ParameterDict()\n",
              "                          (lora_magnitude_vector): ModuleDict()\n",
              "                        )\n",
              "                        (q_proj): lora.Linear4bit(\n",
              "                          (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "                          (lora_dropout): ModuleDict(\n",
              "                            (default): Dropout(p=0.1, inplace=False)\n",
              "                          )\n",
              "                          (lora_A): ModuleDict(\n",
              "                            (default): Linear(in_features=1024, out_features=64, bias=False)\n",
              "                          )\n",
              "                          (lora_B): ModuleDict(\n",
              "                            (default): Linear(in_features=64, out_features=1024, bias=False)\n",
              "                          )\n",
              "                          (lora_embedding_A): ParameterDict()\n",
              "                          (lora_embedding_B): ParameterDict()\n",
              "                          (lora_magnitude_vector): ModuleDict()\n",
              "                        )\n",
              "                        (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "                      )\n",
              "                      (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "                      (mlp): CLIPMLP(\n",
              "                        (activation_fn): QuickGELUActivation()\n",
              "                        (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
              "                        (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
              "                      )\n",
              "                      (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              )\n",
              "            )\n",
              "            (multi_modal_projector): LlavaMultiModalProjector(\n",
              "              (linear_1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
              "              (act): GELUActivation()\n",
              "              (linear_2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
              "            )\n",
              "            (language_model): LlamaModel(\n",
              "              (embed_tokens): Embedding(32064, 4096)\n",
              "              (layers): ModuleList(\n",
              "                (0-31): 32 x LlamaDecoderLayer(\n",
              "                  (self_attn): LlamaAttention(\n",
              "                    (q_proj): lora.Linear4bit(\n",
              "                      (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                      (lora_dropout): ModuleDict(\n",
              "                        (default): Dropout(p=0.1, inplace=False)\n",
              "                      )\n",
              "                      (lora_A): ModuleDict(\n",
              "                        (default): Linear(in_features=4096, out_features=64, bias=False)\n",
              "                      )\n",
              "                      (lora_B): ModuleDict(\n",
              "                        (default): Linear(in_features=64, out_features=4096, bias=False)\n",
              "                      )\n",
              "                      (lora_embedding_A): ParameterDict()\n",
              "                      (lora_embedding_B): ParameterDict()\n",
              "                      (lora_magnitude_vector): ModuleDict()\n",
              "                    )\n",
              "                    (k_proj): lora.Linear4bit(\n",
              "                      (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                      (lora_dropout): ModuleDict(\n",
              "                        (default): Dropout(p=0.1, inplace=False)\n",
              "                      )\n",
              "                      (lora_A): ModuleDict(\n",
              "                        (default): Linear(in_features=4096, out_features=64, bias=False)\n",
              "                      )\n",
              "                      (lora_B): ModuleDict(\n",
              "                        (default): Linear(in_features=64, out_features=4096, bias=False)\n",
              "                      )\n",
              "                      (lora_embedding_A): ParameterDict()\n",
              "                      (lora_embedding_B): ParameterDict()\n",
              "                      (lora_magnitude_vector): ModuleDict()\n",
              "                    )\n",
              "                    (v_proj): lora.Linear4bit(\n",
              "                      (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                      (lora_dropout): ModuleDict(\n",
              "                        (default): Dropout(p=0.1, inplace=False)\n",
              "                      )\n",
              "                      (lora_A): ModuleDict(\n",
              "                        (default): Linear(in_features=4096, out_features=64, bias=False)\n",
              "                      )\n",
              "                      (lora_B): ModuleDict(\n",
              "                        (default): Linear(in_features=64, out_features=4096, bias=False)\n",
              "                      )\n",
              "                      (lora_embedding_A): ParameterDict()\n",
              "                      (lora_embedding_B): ParameterDict()\n",
              "                      (lora_magnitude_vector): ModuleDict()\n",
              "                    )\n",
              "                    (o_proj): lora.Linear4bit(\n",
              "                      (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                      (lora_dropout): ModuleDict(\n",
              "                        (default): Dropout(p=0.1, inplace=False)\n",
              "                      )\n",
              "                      (lora_A): ModuleDict(\n",
              "                        (default): Linear(in_features=4096, out_features=64, bias=False)\n",
              "                      )\n",
              "                      (lora_B): ModuleDict(\n",
              "                        (default): Linear(in_features=64, out_features=4096, bias=False)\n",
              "                      )\n",
              "                      (lora_embedding_A): ParameterDict()\n",
              "                      (lora_embedding_B): ParameterDict()\n",
              "                      (lora_magnitude_vector): ModuleDict()\n",
              "                    )\n",
              "                  )\n",
              "                  (mlp): LlamaMLP(\n",
              "                    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "                    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "                    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "                    (act_fn): SiLU()\n",
              "                  )\n",
              "                  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "                  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "                )\n",
              "              )\n",
              "              (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "          )\n",
              "          (lm_head): Linear(in_features=4096, out_features=32064, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.train()  # ✅ Ensure model is in training mode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSOb88qBWeh_"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True*\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CTSuS7LkNHfI",
        "outputId": "cac4716c-d81c-40af-b7be-b01ca74276a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2945' max='3040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2945/3040 3:20:57 < 06:29, 0.24 it/s, Epoch 19.37/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>3.818700</td>\n",
              "      <td>3.735749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.369800</td>\n",
              "      <td>3.362265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>3.316200</td>\n",
              "      <td>3.318796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.296600</td>\n",
              "      <td>3.298306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>3.296300</td>\n",
              "      <td>3.294652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>3.283500</td>\n",
              "      <td>3.292778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>3.279800</td>\n",
              "      <td>3.292519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>3.287500</td>\n",
              "      <td>3.291946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>3.286900</td>\n",
              "      <td>3.292073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.292800</td>\n",
              "      <td>3.290900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>3.281900</td>\n",
              "      <td>3.324256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>3.292800</td>\n",
              "      <td>3.292118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>3.284300</td>\n",
              "      <td>3.290998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>3.297100</td>\n",
              "      <td>3.291008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>3.295300</td>\n",
              "      <td>3.290380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>3.284700</td>\n",
              "      <td>3.290329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>3.291500</td>\n",
              "      <td>3.290462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>3.293000</td>\n",
              "      <td>3.290349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>3.283700</td>\n",
              "      <td>3.290546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>3.282700</td>\n",
              "      <td>3.291279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>3.291800</td>\n",
              "      <td>3.290677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>3.295200</td>\n",
              "      <td>3.290430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>3.284000</td>\n",
              "      <td>3.290500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>3.276700</td>\n",
              "      <td>3.290417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>3.282800</td>\n",
              "      <td>3.290524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>3.292000</td>\n",
              "      <td>3.290027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>3.294200</td>\n",
              "      <td>3.290258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>3.288100</td>\n",
              "      <td>3.289941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>3.299900</td>\n",
              "      <td>3.290235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>3.283900</td>\n",
              "      <td>3.290520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>3.296800</td>\n",
              "      <td>3.290198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>3.279200</td>\n",
              "      <td>3.290066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>3.267600</td>\n",
              "      <td>3.290443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>3.280500</td>\n",
              "      <td>3.289920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>3.294800</td>\n",
              "      <td>3.290837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>3.288300</td>\n",
              "      <td>3.290512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>3.275600</td>\n",
              "      <td>3.289886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>3.285700</td>\n",
              "      <td>3.289931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>3.289300</td>\n",
              "      <td>3.290093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>3.278600</td>\n",
              "      <td>3.290252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>3.287200</td>\n",
              "      <td>3.290268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>3.286100</td>\n",
              "      <td>3.290177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>3.293400</td>\n",
              "      <td>3.290535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>3.276900</td>\n",
              "      <td>3.290592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>3.280200</td>\n",
              "      <td>3.290864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>3.278900</td>\n",
              "      <td>3.290950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>3.274900</td>\n",
              "      <td>3.290447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>3.299700</td>\n",
              "      <td>3.290797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>3.286700</td>\n",
              "      <td>3.291459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>3.273000</td>\n",
              "      <td>3.290298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>3.290000</td>\n",
              "      <td>3.290655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>3.292900</td>\n",
              "      <td>3.290768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>3.283200</td>\n",
              "      <td>3.290822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>3.280200</td>\n",
              "      <td>3.290668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>3.278300</td>\n",
              "      <td>3.290585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>3.290400</td>\n",
              "      <td>3.290781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>3.284600</td>\n",
              "      <td>3.290705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>3.293300</td>\n",
              "      <td>3.290730</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "ZeNXKFJrvrgN",
        "outputId": "5fee040a-7343-4b8f-8a52-fc0ff4921445"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2474145594.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9fCNmuIicHtj"
      },
      "outputs": [],
      "source": [
        "for log in trainer.state.log_history:\n",
        "    print(log)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhZ-rOU4VipT"
      },
      "outputs": [],
      "source": [
        "print(\"✅ Running evaluation on test set...\")\n",
        "test_metrics = trainer.evaluate(test_dataset)     # ✅ RIGHT\n",
        "print(\"📊 Test set metrics:\", test_metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BU8JK3QcPHh"
      },
      "outputs": [],
      "source": [
        "metrics = trainer.evaluate()\n",
        "print(metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsGs3zHBaQJh"
      },
      "source": [
        "Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvuVZrmNY296"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"./llava-solar-finetuned\")\n",
        "processor.save_pretrained(\"./llava-solar-finetuned\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub_JPmA7aI2i"
      },
      "source": [
        "To keep your model safe, move it to your Google Drive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCk_Mvc1aEqa"
      },
      "outputs": [],
      "source": [
        "!cp -r ./llava-solar-finetuned \"/content/drive/MyDrive/Research/Solar_image\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eX9bBMoaOlA"
      },
      "source": [
        "You can later load it with:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMoQAs31-V2R"
      },
      "source": [
        "IF need load the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8YjnJ7Ucd0M"
      },
      "source": [
        "##Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtEDkUzjgqIK"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# Load image\n",
        "image_path = \"/content/drive/MyDrive/Research/Solar_image/images/dusty/Dust (2).jpg\"\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "display(image)\n",
        "\n",
        "# Define prompt\n",
        "prompt = \"What do you see in this solar panel image?\"\n",
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": prompt}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Format input\n",
        "chat_text = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
        "inputs = processor(image, chat_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate output\n",
        "with torch.no_grad():\n",
        "    output = model.generate(**inputs, max_new_tokens=100)\n",
        "\n",
        "# Decode response\n",
        "decoded = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "response = decoded.split(\"ASSISTANT:\")[-1].strip()\n",
        "\n",
        "# ✅ Print both user input and assistant response\n",
        "print(\"🗨️ User Prompt:\", prompt)\n",
        "print(\"🤖 Model Response:\", response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH7Razhn9sin"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# Load image\n",
        "image_path = \"/content/drive/MyDrive/Research/Solar_image/images/clean/Clean (21).jpg\"\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "display(image)\n",
        "\n",
        "# Define prompt\n",
        "prompt = \"Is there any fault in this solar panel?\"\n",
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": prompt}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Format input\n",
        "chat_text = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
        "inputs = processor(image, chat_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate output\n",
        "with torch.no_grad():\n",
        "    output = model.generate(**inputs, max_new_tokens=100)\n",
        "\n",
        "# Decode response\n",
        "decoded = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "response = decoded.split(\"ASSISTANT:\")[-1].strip()\n",
        "\n",
        "# ✅ Print both user input and assistant response\n",
        "print(\"🗨️ User Prompt:\", prompt)\n",
        "print(\"🤖 Model Response:\", response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYGK54YR-wHY"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from IPython.display import display, Markdown\n",
        "import torch\n",
        "\n",
        "# Load image\n",
        "image_path = \"/content/drive/MyDrive/Research/Solar_image/images/physical damage/crack_083_jpg.rf.946fef3302b7c58ab3c0e3cbaabef6fb.jpg\"\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "# Display image\n",
        "display(image)\n",
        "\n",
        "# Define prompt\n",
        "prompt = \"What is the fault in this solar panel image?\"\n",
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": prompt}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Format input for the model\n",
        "chat_text = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
        "inputs = processor(image, chat_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate output\n",
        "with torch.no_grad():\n",
        "    output = model.generate(**inputs, max_new_tokens=100)\n",
        "\n",
        "# Decode and print response\n",
        "decoded = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "response = decoded.split(\"ASSISTANT:\")[-1].strip()\n",
        "\n",
        "# Display prompt and response\n",
        "display(Markdown(f\"**🗨️ User Prompt:** {prompt}\"))\n",
        "display(Markdown(f\"**🤖 Model Response:** {response}\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3WnvXw1-6BR"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from IPython.display import display, Markdown\n",
        "import torch\n",
        "\n",
        "# Load image\n",
        "image_path = \"/content/drive/MyDrive/Research/Solar_image/images/physical damage/71_jpg.rf.a6d05c0002d3a5789af082f67ebc5263.jpg\"\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "# Display image\n",
        "display(image)\n",
        "\n",
        "# Define prompt\n",
        "prompt = \"What is the fault in this solar panel image?\"\n",
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": prompt}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Format input for the model\n",
        "chat_text = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
        "inputs = processor(image, chat_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate output\n",
        "with torch.no_grad():\n",
        "    output = model.generate(**inputs, max_new_tokens=200)\n",
        "\n",
        "# Decode and print response\n",
        "decoded = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "response = decoded.split(\"ASSISTANT:\")[-1].strip()\n",
        "\n",
        "# Display prompt and response\n",
        "display(Markdown(f\"**🗨️ User Prompt:** {prompt}\"))\n",
        "display(Markdown(f\"**🤖 Model Response:** {response}\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hu7vMe61fcH"
      },
      "source": [
        "##Conversational chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H7aTxuM1yXN"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from IPython.display import display, Markdown\n",
        "import torch\n",
        "\n",
        "# Load the first image (used only once)\n",
        "image_path = \"/content/drive/MyDrive/Research/Solar_image/images/physical damage/71_jpg.rf.a6d05c0002d3a5789af082f67ebc5263.jpg\"\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "display(image)\n",
        "\n",
        "# Define initial conversation\n",
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},  # Only in the first user turn\n",
        "            {\"type\": \"text\", \"text\": \"What is the fault in this solar panel image?\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# === Conversation Loop ===\n",
        "while True:\n",
        "    # Format input with chat history and generate response\n",
        "    chat_text = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = processor(image, chat_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs, max_new_tokens=200)\n",
        "\n",
        "    decoded = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    response = decoded.split(\"ASSISTANT:\")[-1].strip()\n",
        "\n",
        "    # Display model response\n",
        "    display(Markdown(f\"**🤖 Model Response:** {response}\"))\n",
        "\n",
        "    # Add model response to conversation\n",
        "    conversation.append({\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": [{\"type\": \"text\", \"text\": response}]\n",
        "    })\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZFgXe4PBc7F"
      },
      "outputs": [],
      "source": [
        "# Ask for next user input\n",
        "    follow_up = input(\"🗨️ You: \").strip()\n",
        "    if follow_up.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "\n",
        "    # Add new user question (no image after first round)\n",
        "    conversation.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [{\"type\": \"text\", \"text\": follow_up}]\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sQ9wKASDcA3"
      },
      "source": [
        "##Save the model to huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4s8VhLSGOX8"
      },
      "source": [
        "➡️ It will ask you to paste your HF token\n",
        "👉 Get it from: https://huggingface.co/settings/tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co4Rf-xKETYS"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTsPVA5kEaD9"
      },
      "source": [
        "Go to https://huggingface.co/new\n",
        "Set:\n",
        "\n",
        "Model repo name: llava-solar-finetuned\n",
        "\n",
        "Visibility: Private (or Public if you want)\n",
        "\n",
        "Copy your repo name — e.g., your-username/llava-solar-finetuned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2meirBvaEhCg"
      },
      "source": [
        "Push Your Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uv5KaRpcEnIN"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import create_repo, upload_folder\n",
        "\n",
        "repo_id = \"your-username/llava-solar-finetuned\"\n",
        "\n",
        "# OPTIONAL: If you haven't created it manually\n",
        "create_repo(repo_id, private=True)\n",
        "\n",
        "# Upload LoRA adapter + processor (saved after training)\n",
        "upload_folder(\n",
        "    repo_id=repo_id,\n",
        "    folder_path=\"./llava-solar-finetuned\",\n",
        "    path_in_repo=\".\",\n",
        "    commit_message=\"Upload fine-tuned LLaVA solar fault model\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lpTXxWtEtUa"
      },
      "source": [
        "Load It Later in Any Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXJTLeT0Ev6U"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor\n",
        "from peft import PeftModel\n",
        "from transformers import LlavaForConditionalGeneration, BitsAndBytesConfig\n",
        "\n",
        "# Load base model\n",
        "base_model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "model = LlavaForConditionalGeneration.from_pretrained(\n",
        "    base_model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load processor and LoRA adapter from Hugging Face\n",
        "repo_id = \"your-username/llava-solar-finetuned\"\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "snapshot_download(repo_id)  # Optional to force download\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(repo_id)\n",
        "model = PeftModel.from_pretrained(model, repo_id)\n",
        "\n",
        "print(\"✅ Model loaded from Hugging Face successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYlrvFxm11I2"
      },
      "source": [
        "##Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1bTjlvyAULu"
      },
      "outputs": [],
      "source": [
        "!pip install gradio --quiet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKcF9K85AbgT"
      },
      "source": [
        "Define memory + response function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqeC64rkAd5e"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# 🧠 Global memory to store current image and chat history\n",
        "current_image = None\n",
        "chat_history = []\n",
        "\n",
        "# 🧠 Define chatbot function\n",
        "def solar_fault_chat(image, user_input, history):\n",
        "    global current_image, chat_history\n",
        "\n",
        "    # If new image is uploaded, reset chat history\n",
        "    if image is not None and image != current_image:\n",
        "        current_image = image\n",
        "        chat_history = []  # Clear chat history for new image\n",
        "\n",
        "    # Build conversation\n",
        "    conversation = []\n",
        "\n",
        "    for user_msg, bot_msg in chat_history:\n",
        "        conversation.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": user_msg}]\n",
        "        })\n",
        "        conversation.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": bot_msg}]\n",
        "        })\n",
        "\n",
        "    # Add the current user input\n",
        "    conversation.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": user_input}]\n",
        "    })\n",
        "\n",
        "    # Format input using tokenizer’s chat template\n",
        "    chat_text = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # Prepare inputs for model\n",
        "    inputs = processor(image, chat_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs, max_new_tokens=150)\n",
        "\n",
        "    # Decode response\n",
        "    decoded = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    response = decoded.split(\"ASSISTANT:\")[-1].strip()\n",
        "\n",
        "    # Update history\n",
        "    chat_history.append((user_input, response))\n",
        "\n",
        "    return response, chat_history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YqSmHyxAj04"
      },
      "source": [
        "Create Gradio Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb760HfmAmDY"
      },
      "outputs": [],
      "source": [
        "# Gradio Blocks Interface for chat + image\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## ☀️ Solar Panel Fault Diagnosis Chatbot\")\n",
        "\n",
        "    with gr.Row():\n",
        "        image_input = gr.Image(type=\"pil\", label=\"Upload Solar Panel Image\")\n",
        "        chatbot = gr.Chatbot()\n",
        "\n",
        "    with gr.Row():\n",
        "        user_input = gr.Textbox(label=\"Ask your question about the image\")\n",
        "        submit_btn = gr.Button(\"Send\")\n",
        "\n",
        "    # When button clicked, call solar_fault_chat\n",
        "    def chat_wrapper(img, msg, history):\n",
        "        response, updated_history = solar_fault_chat(img, msg, history)\n",
        "        return \"\", updated_history\n",
        "\n",
        "    submit_btn.click(chat_wrapper, inputs=[image_input, user_input, chatbot], outputs=[user_input, chatbot])\n",
        "\n",
        "demo.launch(share=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}